# IMAGE-CAPTIONING-
Image captioning is a challenging problem that bridges computer vision and natural language processing. This project focuses on designing and developing an AI-powered system that automatically generates captions for images using neural networks. 

By leveraging Deep Learning Techniques such as BLIP (Bootstrapped Language-Image Pretraining) model, which uses a Vision Transformer (ViT) for image encoding and a Transformer- based decoder for caption generation for sequence generation, the system aims to generate meaningful textual descriptions of images. The project involves data preprocessing, model training, evaluation, and deployment to create an efficient and scalable image captioning system.
The project follows a structured pipeline involving:
1.Data Collection & Preprocessing – Using publicly available datasets like MS COCO and Flickr30k.
2.Model Selection & Training – Fine-tuning BLIP for optimal caption generation.
3.Feature Extraction & Evaluation – Using Vision Transformer (ViT) for object detection and evaluating the system with BLEU, CIDEr, ROUGE, and SPICE scores.
4.Deployment as a Web Application – Implementing the model using Streamlit, allowing users to upload images and receive AI-generated captions in real-time.
